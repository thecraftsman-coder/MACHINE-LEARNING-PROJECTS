{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e72889-a278-47b7-a510-be91eb316afd",
   "metadata": {},
   "source": [
    "- Import you data and perform basic data exploration phase\n",
    "- Display general information about the dataset\n",
    "- Create a pandas profiling reports to gain insights into the dataset\n",
    "- Handle Missing and corrupted values\n",
    "- Remove duplicates, if they exist\n",
    "- Handle outliers, if they exist\n",
    "- Encode categorical features\n",
    "- Select your target variable and the features\n",
    "- Split your dataset to training and test sets\n",
    "- Based on your data exploration phase select a ML regression algorithm and train it on the training set\n",
    "- Assess your model performance on the test set using relevant evaluation metrics\n",
    "- Discuss with your cohort alternative ways to improve your model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4588f9-5c4e-433b-bd58-bcd56a187486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca32e1-8117-449f-b541-43471332c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For scaling numerical data and encoding categorical data\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet  # For linear Regression\n",
    "from sklearn.tree import DecisionTreeRegressor  # For Decision Tree Regression\n",
    "from sklearn.ensemble import RandomForestRegressor  # For Random Forest Regression\n",
    "from sklearn.svm import SVR  # For Support Vector Regression \n",
    "import xgboost as xgb # For XGBoost Regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer  # For model evaluation metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738590f1-89c4-4427-9395-b1a437962be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = pd.read_csv(\"5G_energy_consumption_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3780d1-c973-45f3-8ed6-c9540367b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2949af-d0d5-4ce6-8d52-c39fc717a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df[\"Time\"] = pd.to_datetime(energy_df[\"Time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e336b-a693-48b1-a90c-59da8ab98c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e218e-4c1d-4e3f-b5ce-da531f6cac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe25c4-079e-45c3-9f91-2aae81e6f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb2574-d058-4485-aa1b-d1d851068c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5bc9d2-780f-4583-aac0-bae0668de262",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(energy_df, title = \"Energy consumption Report\", explorative = True)\n",
    "profile.to_file(\"energy_profile_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2662705-4820-4ba6-b321-b7eaac7f5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c53422-c48b-4818-a1e5-612d32bef186",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = energy_df[energy_df[\"ESMODE\"] != 0.0]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b08fa3-b979-4788-8692-ff8013c218c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = energy_df.drop(\"ESMODE\", axis=1)\n",
    "energy_df = energy_df.drop(\"Time\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06cd46-e361-4d1f-bfd8-839ae16c5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264b177-f0ea-4283-a77d-7df1caf4b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f8590-235a-484d-8161-98958476295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "energy_df[\"BS\"] = le.fit_transform(energy_df[\"BS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586637bc-8c31-43aa-9b72-f9cd319fb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df[\"BS\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8fdfe-9904-42dd-b70e-43febee619c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7560876-4c88-40e0-af22-cf9a6300d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features (columns)\n",
    "%matplotlib inline\n",
    "numerical_features = energy_df.select_dtypes(include='number').columns\n",
    "\n",
    "# Create a grid of subplots for box plots\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(len(numerical_features)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x=energy_df[numerical_features[i]], palette='viridis')\n",
    "    plt.title(numerical_features[i], fontsize=14)  # Adjust font size if needed\n",
    "    plt.xlabel(' ')\n",
    "plt.tight_layout()  # Call this function outside the loop\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c489c09-57a6-47c1-ab41-b109f844e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = energy_df.drop(columns=['Energy'])  # Drop the target column to get features\n",
    "y = energy_df[\"Energy\"]  # Select the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2f76-6682-4856-a58b-635e21780b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3da2f2-f415-4210-9200-0717b44a40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6327e-3fb1-4cf5-8246-668514103a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_lr = lin_reg.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Linear Regression RMSE:\", mean_squared_error(y_test, y_pred_lr, squared=False))\n",
    "print(\"Linear Regression R2 Score:\", r2_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703b271-f39e-4ed4-84c5-f6cac9ae26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize the Elastic Net model\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_en = elastic_net.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Elastic Net RMSE:\", mean_squared_error(y_test, y_pred_en, squared=False))\n",
    "print(\"Elastic Net R2 Score:\", r2_score(y_test, y_pred_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c50e04-62ed-4c08-ba86-d6778891a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with some parameters\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5, min_samples_split=10, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "tree_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_tree = tree_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Decision Tree RMSE:\", mean_squared_error(y_test, y_pred_tree, squared=False))\n",
    "print(\"Decision Tree R2 Score:\", r2_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8e920-9043-4c90-a02f-92f74735e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with some parameters\n",
    "svr_reg = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
    "\n",
    "# Fit the model\n",
    "svr_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_svr = svr_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"SVR RMSE:\", mean_squared_error(y_test, y_pred_svr, squared=False))\n",
    "print(\"SVR R2 Score:\", r2_score(y_test, y_pred_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365b0e7-b291-4b66-bfc6-a53b94dd818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with some parameters\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_rf = rf_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Random Forest RMSE:\", mean_squared_error(y_test, y_pred_rf, squared=False))\n",
    "print(\"Random Forest R2 Score:\", r2_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4789a7-0c9a-4d90-a882-5eda5a9da8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with some parameters\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=6)\n",
    "\n",
    "# Fit the model\n",
    "xgb_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_xgb = xgb_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"XGBoost RMSE:\", mean_squared_error(y_test, y_pred_xgb, squared=False))\n",
    "print(\"XGBoost R2 Score:\", r2_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569654d7-c6e8-4684-86b5-7fd8f7e58a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use XGBoost and tune the n_estimators, learning_rate, and max_depth.\n",
    "\n",
    "# Initialize the best RMSE and best R² to extreme values to ensure any calculated values will be better\n",
    "best_rmse = float('inf')\n",
    "best_r2 = -float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over different values for n_estimators, learning_rate, and max_depth\n",
    "for n_estimators in [50, 100, 200]:  # Number of boosting rounds\n",
    "    for learning_rate in [0.01, 0.1, 0.2]:  # Step size at each iteration\n",
    "        for max_depth in range(3, 10, 2):  # Maximum depth of each tree\n",
    "\n",
    "            # Initialize the XGBoost model with the current set of hyperparameters\n",
    "            xgb_reg = xgb.XGBRegressor(\n",
    "                n_estimators=n_estimators,       # Number of boosting rounds\n",
    "                learning_rate=learning_rate,     # Step size at each iteration\n",
    "                max_depth=max_depth,             # Maximum depth of each tree\n",
    "                random_state=42                  # Ensures reproducibility\n",
    "            )\n",
    "            \n",
    "            # Train the model using the training data\n",
    "            xgb_reg.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict the target values for the test data\n",
    "            y_pred_xgb = xgb_reg.predict(X_test)\n",
    "            \n",
    "            # Calculate Root Mean Squared Error (RMSE) and R² score for the current model\n",
    "            rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "            r2 = r2_score(y_test, y_pred_xgb)\n",
    "            \n",
    "            # Check if the current RMSE is better (lower) than the best RMSE so far\n",
    "            if rmse < best_rmse:\n",
    "                # Update the best RMSE, R² score, and the best parameters\n",
    "                best_rmse = rmse\n",
    "                best_r2 = r2\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators, \n",
    "                    'learning_rate': learning_rate, \n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "\n",
    "# Print the best hyperparameters and corresponding RMSE and R² score\n",
    "print(\"Best Parameters for XGBoost:\", best_params)\n",
    "print(\"Best RMSE for XGBoost:\", best_rmse)\n",
    "print(\"Best R² Score for XGBoost:\", best_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c9594-f48a-486f-8b64-8349814cb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions from scikit-learn\n",
    "from sklearn.metrics import make_scorer  # To create custom scoring functions\n",
    "from sklearn.model_selection import cross_validate  # To perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3614017-96c7-45bb-8b38-6acd30bc221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "\n",
    "# Create custom scorers for RMSE and R²\n",
    "# `make_scorer` allows using custom metrics or built-in metrics in cross-validation\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Dictionary of scoring metrics\n",
    "scoring = {'RMSE': rmse_scorer, 'R2': r2_scorer}\n",
    "\n",
    "# Perform cross-validation\n",
    "# `cross_validate` splits the data into folds, trains and tests the model, and calculates the scores\n",
    "cv_results = cross_validate(model, X, y, scoring=scoring, cv=5, return_train_score=True)\n",
    "# Note that we are using X and y and not X_train and y_train\n",
    "\n",
    "# Output the results\n",
    "# `cv_results` contains the scores for each fold\n",
    "print(\"RMSE scores:\", cv_results['test_RMSE'])  # RMSE scores for each fold\n",
    "print(\"R² scores:\", cv_results['test_R2'])  # R² scores for each fold\n",
    "print(\"Average RMSE:\", cv_results['test_RMSE'].mean())  # Average RMSE across all folds\n",
    "print(\"Average R²:\", cv_results['test_R2'].mean())  # Average R² score across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fb64c-4259-44a6-89d7-84617c56d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use XGBoost and tune the n_estimators, learning_rate, and max_depth.\n",
    "\n",
    "# Initialize the best RMSE and best R² to extreme values to ensure any calculated values will be better\n",
    "best_rmse = float('inf')\n",
    "best_r2 = -float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over different values for n_estimators, learning_rate, and max_depth\n",
    "for n_estimators in [50, 100, 200]:  # Number of boosting rounds\n",
    "    for learning_rate in [0.01, 0.1, 0.2]:  # Step size at each iteration\n",
    "        for max_depth in range(3, 10, 2):  # Maximum depth of each tree\n",
    "\n",
    "            # Initialize the XGBoost model with the current set of hyperparameters\n",
    "            xgb_reg = xgb.XGBRegressor(\n",
    "                n_estimators=n_estimators,       # Number of boosting rounds\n",
    "                learning_rate=learning_rate,     # Step size at each iteration\n",
    "                max_depth=max_depth,             # Maximum depth of each tree\n",
    "                random_state=42                  # Ensures reproducibility\n",
    "            )\n",
    "            \n",
    "            # Train the model using the training data\n",
    "            xgb_reg.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict the target values for the test data\n",
    "            y_pred_xgb = xgb_reg.predict(X_test)\n",
    "            \n",
    "            # Calculate Root Mean Squared Error (RMSE) and R² score for the current model\n",
    "            rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "            r2 = r2_score(y_test, y_pred_xgb)\n",
    "            \n",
    "            # Check if the current RMSE is better (lower) than the best RMSE so far\n",
    "            if rmse < best_rmse:\n",
    "                # Update the best RMSE, R² score, and the best parameters\n",
    "                best_rmse = rmse\n",
    "                best_r2 = r2\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators, \n",
    "                    'learning_rate': learning_rate, \n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "\n",
    "# Print the best hyperparameters and corresponding RMSE and R² score\n",
    "print(\"Best Parameters for XGBoost:\", best_params)\n",
    "print(\"Best RMSE for XGBoost:\", best_rmse)\n",
    "print(\"Best R² Score for XGBoost:\", best_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af100a7-76f9-4031-8bd1-29dcbfd82eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model with default settings\n",
    "xgb_model = xgb.XGBRegressor(booster='gbtree', objective='reg:squarederror')\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.15, 0.3, 0.5],  # Learning rate (step size)\n",
    "    'n_estimators': [100, 500, 1000, 2000, 3000],  # Number of boosting rounds\n",
    "    'max_depth': [3, 6, 9],  # Maximum depth of trees\n",
    "    'min_child_weight': [1, 5, 10, 20],  # Minimum sum of instance weight\n",
    "    'reg_alpha': [0.001, 0.01, 0.1],  # L1 regularization term\n",
    "    'reg_lambda': [0.001, 0.01, 0.1]  # L2 regularization term\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "xgb_reg = GridSearchCV(\n",
    "    estimator=xgb_model,  # Model to use\n",
    "    param_grid=param_grid,  # Parameter grid to search through\n",
    "    scoring='neg_root_mean_squared_error',  # Scoring metric for evaluation\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    verbose=1,  # Print progress messages\n",
    "    n_jobs=-1  # Use all available cores for parallel processing\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "xgb_search = xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = xgb_search.best_params_\n",
    "\n",
    "# Initialize the XGBoost model with the best hyperparameters\n",
    "xgb_best = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "print(\"Best Parameters for XGBoost:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
